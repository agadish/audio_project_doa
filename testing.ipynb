{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.load('outputs_torch.pt').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['samples', 'ref_stft', 'target', 'perceived_signals', 'doas'])\n",
      "samples.shape=torch.Size([64, 14, 256, 96])\n",
      "ref_stft.shape=torch.Size([64, 257, 96])\n",
      "target.shape=torch.Size([64, 256, 96])\n",
      "perceived_signals.shape=torch.Size([64, 2, 12160])\n",
      "doas.shape=torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "d = torch.load('data_batches/train06r076_37.pt')\n",
    "print(d.keys())\n",
    "samples, ref_stft, target, perceived_signals, doas = d['samples'], d['ref_stft'], d['target'], d['perceived_signals'], d['doas']\n",
    "print(f\"samples.shape={samples.shape}\")\n",
    "print(f\"ref_stft.shape={ref_stft.shape}\")\n",
    "print(f\"target.shape={target.shape}\")\n",
    "print(f\"perceived_signals.shape={perceived_signals.shape}\")\n",
    "print(f\"doas.shape={doas.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 14, 256, 1120]), torch.Size([2, 257, 1120]), torch.Size([2, 256, 1120]), torch.Size([2, 2, 149119]), torch.Size([2, 2])]\n"
     ]
    }
   ],
   "source": [
    "d1 = {'samples': samples[:2], 'ref_stft': ref_stft[:2], 'target': target[:2], 'mixed_signals': perceived_signals[:2], 'doas': doas[:2]}\n",
    "print([v.shape for k,v in d1.items()])\n",
    "torch.save(d1, 'example_batch2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape=torch.Size([8, 13, 256, 1120])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "angles_idx = torch.argmax(probs, dim=1)\n",
    "vals = angles_idx[0].unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([ 1063,  2157,  4026,  5348,  7552, 11676, 13289, 14329, 31553, 37984,\n",
      "        50508, 53235, 54000]),\n",
      "indices=tensor([ 9, 10, 11,  7,  2,  5,  3,  0,  1,  8, 12,  4,  6]))\n"
     ]
    }
   ],
   "source": [
    "print(vals[1].sort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bsseval\n",
    "\n",
    "class SeparatedSource:\n",
    "\t\"\"\"Represents the part of the received sound that comes from this\n",
    "\tspecific source (angle).\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, ref_spec, probs):\n",
    "\t\t# The ref mic's spectrogram, shape=(Bmw, t)\n",
    "\t\tself.ref_spec = ref_spec\n",
    "\t\t# The model's output for this angle, shape=(w, t)\n",
    "\t\tself.probs = probs\n",
    "\t\n",
    "\tdef energy(self):\n",
    "\t\treturn torch.sum(self.probs * abs(self.ref_spec) ** 2)\n",
    "\t\n",
    "\tdef spec(self):\n",
    "\t\tmag = abs(self.ref_spec) * self.probs\n",
    "\t\tphase = torch.angle(self.ref_spec)\n",
    "\t\treturn mag * torch.exp(1j * phase)\n",
    "\t\n",
    "\tdef metrics(self):\n",
    "\t\t\"\"\"Retuns SDR, ISR, SIR, SAR.\"\"\"\n",
    "\t\tsep_signal = torch.istft(self.spec(), ...)  # <-- Finish me\n",
    "\t\t# TODO: maybe use the reference time signal directly?\n",
    "\t\tref_signal = torch.istft(self.ref_spec, ...)  # <-- Finish me\n",
    "\t\t\n",
    "\t\treturn bsseval.evaluate(\n",
    "\t\t\treferences=sep_signal.reshape(...),  # <-- Finish me\n",
    "\t\t\testimates=ref_signal.reshape(...),  # <-- Finish me\n",
    "\t\t\t# win=1*44100,\n",
    "\t\t\t# hop=1*44100,\n",
    "\t\t\t# mode='v4',\n",
    "\t\t\t# padding=True\n",
    "\t\t)\n",
    "\t\n",
    "\t@classmethod\n",
    "\tdef speaker_angles(cls, sources):\n",
    "\t\t\"\"\"Returns the 2 angle numbers where the speakers are (most\n",
    "\t\tlikely) located.\n",
    "\t\t\"\"\"\n",
    "\t\tenergies = [src.energy() for src in sources]\n",
    "\t\t# `argpartition` to get the indices, AKA angle numbers.\n",
    "\t\tpartitioned = np.argpartition(energies, angle_count - 2)\n",
    "\t\tmax_angles = tuple(partitioned[-2:])\n",
    "\t\treturn max_angles\n",
    "\t\n",
    "\t@classmethod\n",
    "\tdef sample_metrics(cls, ref_spec, samp_probs):\n",
    "\t\t\"\"\"Retuns the metrics for this sample's data & label.\"\"\"\n",
    "\t\t# `samp_probs`'s shape is (angle_count, w, t).\n",
    "\t\t# `sources[i]` is the separeted source coming from the direction\n",
    "\t\t# theta_i.\n",
    "\t\tsources = [SeparatedSource(ref_spec, probs) for probs in samp_probs]\n",
    "\t\t\n",
    "\t\tspeaker_angles = SeparatedSource.speaker_angles(sources)\n",
    "\t\tspeaker_metrics = [\n",
    "\t\t\tsources[angle.spec].metrics()\n",
    "\t\t\tfor angle in speaker_angles\n",
    "\t\t]\n",
    "\t\treturn speaker_metrics\n",
    "\n",
    "# Loop for each sample in the batch\n",
    "# for ref_spec, samp_probs in zip(batch_ref_specs, batch_outputs):\n",
    "\t# print(SeparatedSource.sample_metrics(ref_spec, samp_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1120])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_stft[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3090761., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.view(list(probs.shape)[:-2] + [-1]) @ torch.abs(ref_stft)\n",
    "torch.sum(probs * abs(ref_stft[1:]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeparatedSource(target, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_proj_generator_v2 import TrainLibriSpeechAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_name</th>\n",
       "      <th>record_name</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>reverb_time</th>\n",
       "      <th>SIR</th>\n",
       "      <th>theta</th>\n",
       "      <th>radius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>15664</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\3000\\1566...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.532481</td>\n",
       "      <td>180</td>\n",
       "      <td>1.185416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6345</td>\n",
       "      <td>64257</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\6345\\6425...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.280003</td>\n",
       "      <td>135</td>\n",
       "      <td>2.390319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5694</td>\n",
       "      <td>64029</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\5694\\6402...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.999142</td>\n",
       "      <td>60</td>\n",
       "      <td>1.786532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842</td>\n",
       "      <td>302203</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\8842\\3022...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.769222</td>\n",
       "      <td>150</td>\n",
       "      <td>1.344866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1462</td>\n",
       "      <td>170142</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\1462\\1701...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.778789</td>\n",
       "      <td>120</td>\n",
       "      <td>1.955771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>8297</td>\n",
       "      <td>275154</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\8297\\2751...</td>\n",
       "      <td>1021</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.338972</td>\n",
       "      <td>30</td>\n",
       "      <td>1.362223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2078</td>\n",
       "      <td>142845</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\2078\\1428...</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.326748</td>\n",
       "      <td>135</td>\n",
       "      <td>1.566237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>6295</td>\n",
       "      <td>244435</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\6295\\2444...</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.349233</td>\n",
       "      <td>30</td>\n",
       "      <td>1.677237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>6241</td>\n",
       "      <td>61943</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\6241\\6194...</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.940246</td>\n",
       "      <td>150</td>\n",
       "      <td>1.701565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2902</td>\n",
       "      <td>9008</td>\n",
       "      <td>source_signals/LibriSpeech/dev-clean\\2902\\9008...</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.389571</td>\n",
       "      <td>150</td>\n",
       "      <td>1.431728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      speaker_name  record_name  \\\n",
       "0             3000        15664   \n",
       "1             6345        64257   \n",
       "2             5694        64029   \n",
       "3             8842       302203   \n",
       "4             1462       170142   \n",
       "...            ...          ...   \n",
       "2043          8297       275154   \n",
       "2044          2078       142845   \n",
       "2045          6295       244435   \n",
       "2046          6241        61943   \n",
       "2047          2902         9008   \n",
       "\n",
       "                                             audio_path  pair_id  reverb_time  \\\n",
       "0     source_signals/LibriSpeech/dev-clean\\3000\\1566...        0          0.4   \n",
       "1     source_signals/LibriSpeech/dev-clean\\6345\\6425...        0          0.2   \n",
       "2     source_signals/LibriSpeech/dev-clean\\5694\\6402...        1          0.3   \n",
       "3     source_signals/LibriSpeech/dev-clean\\8842\\3022...        1          0.4   \n",
       "4     source_signals/LibriSpeech/dev-clean\\1462\\1701...        2          0.4   \n",
       "...                                                 ...      ...          ...   \n",
       "2043  source_signals/LibriSpeech/dev-clean\\8297\\2751...     1021          0.2   \n",
       "2044  source_signals/LibriSpeech/dev-clean\\2078\\1428...     1022          0.2   \n",
       "2045  source_signals/LibriSpeech/dev-clean\\6295\\2444...     1022          0.2   \n",
       "2046  source_signals/LibriSpeech/dev-clean\\6241\\6194...     1023          0.4   \n",
       "2047  source_signals/LibriSpeech/dev-clean\\2902\\9008...     1023          0.3   \n",
       "\n",
       "           SIR  theta    radius  \n",
       "0    -0.532481    180  1.185416  \n",
       "1     1.280003    135  2.390319  \n",
       "2     1.999142     60  1.786532  \n",
       "3    -0.769222    150  1.344866  \n",
       "4     1.778789    120  1.955771  \n",
       "...        ...    ...       ...  \n",
       "2043  0.338972     30  1.362223  \n",
       "2044  1.326748    135  1.566237  \n",
       "2045 -1.349233     30  1.677237  \n",
       "2046  0.940246    150  1.701565  \n",
       "2047  1.389571    150  1.431728  \n",
       "\n",
       "[2048 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = TrainLibriSpeechAudio()\n",
    "df = gen.generate_dataset_meta()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform.shape=torch.Size([1, 222560])\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.transforms import Spectrogram, InverseSpectrogram\n",
    "import torchaudio\n",
    "\n",
    "FRAME_LENGTH = 512 # K\n",
    "NFFT = FRAME_LENGTH  # FFT length in STFT\n",
    "OVERLAP = 0.75  # Frame overlap in STFT\n",
    "HOP_LENGTH = int((1 - OVERLAP) * NFFT)  # Hop length in STFTHOP_LENGTH = \n",
    "FS = 16e3\n",
    "\n",
    "waveform, sample_rate = torchaudio.load('source_signals/LibriSpeech/train-clean-100/27/124992/27-124992-0001.flac')\n",
    "waveform = waveform.cuda()\n",
    "print(f\"waveform.shape={waveform.shape}\")\n",
    "\n",
    "spectrogram = Spectrogram(\n",
    "    n_fft=NFFT,\n",
    "    win_length=NFFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    power=None  # Complex spectrum\n",
    ").to('cuda')\n",
    "\n",
    "\n",
    "ispectrogram = InverseSpectrogram(\n",
    "    n_fft=NFFT,\n",
    "    win_length=NFFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    ").to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_len(waveform, len):\n",
    "    wfl = waveform[:, :len]\n",
    "    stft = spectrogram(wfl)\n",
    "    istft = ispectrogram(stft)\n",
    "    print(f\"original shape={wfl.shape}\")\n",
    "    print(f\"stft shape={stft.shape}\")\n",
    "    print(f\"istft(stft) shape={istft.shape}\")\n",
    "    print(f\"stft last dim: {stft.size(-1)}, align 16 down: {stft.size(-1) // 16 * 16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape=torch.Size([1, 34688])\n",
      "stft shape=torch.Size([1, 257, 272])\n",
      "istft(stft) shape=torch.Size([1, 34688])\n",
      "stft last dim: 272, align 16 down: 272\n",
      "orig1271//16 * 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163760.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_len(waveform, int(16000*2.168))\n",
    "print(f\"orig1271//16 * 16\")\n",
    "16000*10.235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape=torch.Size([1, 12220])\n",
      "stft shape=torch.Size([1, 257, 96])\n",
      "istft(stft) shape=torch.Size([1, 12160])\n"
     ]
    }
   ],
   "source": [
    "print_len(waveform, 12220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1152/1152 [01:24<00:00, 13.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from audio_dataset import DictTorchPartedDataset\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda'\n",
    "train_dataset = DictTorchPartedDataset('data_batches', 'train06r076' , ['samples', 'ref_stft', 'target'], real_batch_size=64, virtual_batch_size=1, device=device)\n",
    "\n",
    "for i in tqdm(range(4864, len(train_dataset))):\n",
    "    samples, _, _ = train_dataset[i]\n",
    "    if torch.any(torch.isnan(samples)):\n",
    "        print(f\"{i} has {len(torch.isnan(samples) / samples.numel() * 100)}% nans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_batches/__SSS__test10tgtprc_0.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m d \u001b[38;5;241m=\u001b[39m {k:v[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{i} : _{i // 64}.pt, member {i % 64}\")\n",
    "\n",
    "data = torch.load('data_batches/train06r076_75.pt')\n",
    "d = {k:v[30] for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples.shape=torch.Size([14, 256, 96])\n",
      "ref_stft.shape=torch.Size([257, 96])\n",
      "target.shape=torch.Size([256, 96])\n",
      "perceived_signals.shape=torch.Size([2, 12160])\n",
      "doas.shape=torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "samples, ref_stft, target, perceived_signals, doas = d['samples'], d['ref_stft'], d['target'], d['perceived_signals'], d['doas']\n",
    "print(f\"samples.shape={samples.shape}\")\n",
    "print(f\"ref_stft.shape={ref_stft.shape}\")\n",
    "print(f\"target.shape={target.shape}\")\n",
    "print(f\"perceived_signals.shape={perceived_signals.shape}\")\n",
    "print(f\"doas.shape={doas.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perceived_signals[1][torch.isnan(perceived_signals[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.load('data_batches/__SSS__test10tgtprc_0.pt')\n",
    "d = {k: torch.tensor(v[:2].detach().cpu().numpy()) for k,v in data.items()}\n",
    "d['probs'] = torch.tensor(probs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(d, 'samples_test1405.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(array([[112]], dtype=uint8), array([[256]], dtype=uint16), array([[1]], dtype=uint8), array([[3]], dtype=uint8), array([[48000]], dtype=uint16), array([[1, 2, 3, 4, 5, 6, 7, 8]], dtype=uint8), array([[624]], dtype=uint16), array([[4]], dtype=uint8), array([[6]], dtype=uint8), array([[0]], dtype=uint8), array([[1]], dtype=uint8), array([], dtype='<U1'), array([[0.5]]), array([[0]], dtype=uint8), array([[10]], dtype=uint8), array([[0]], dtype=uint8))]]\n",
      "[[(array(['Reverberation_0.160s'], dtype='<U20'), array([[0]], dtype=uint8), array([[1]], dtype=uint8), array(['PerfectSweep'], dtype='<U12'), array([[(array(['RME Hammerfall DSP Digiface + ADI-8 DS'], dtype='<U38'), array(['RME Octamic'], dtype='<U11'), array(['Fostex 6301B and ADAM A3X'], dtype='<U25'), array(['8x AKG CK32'], dtype='<U11'))]],\n",
      "        dtype=[('soundcard', 'O'), ('micpreamp', 'O'), ('loudspeaker', 'O'), ('microphones', 'O')]), array(['Bar-Ilan_University'], dtype='<U19'), array([[0.78190921, 0.87078178, 0.79240494, 0.80242944, 0.74739788,\n",
      "          0.97152344, 1.        , 0.96765336]]), array(['mic1@90deg, mic8@270deg'], dtype='<U23'), array([[3, 3, 3, 8, 3, 3, 3]], dtype=uint8), array(['covr'], dtype='<U4'), array([[0.16]]), array(['Acoustic_Lab_Bar-Ilan_University'], dtype='<U32'), array(['impulse_response'], dtype='<U16'))                                                       ]]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "mat_path = 'timit_rir/Impulse_response_Acoustic_Lab_Bar-Ilan_University_(Reverberation_0.160s)_3-3-3-8-3-3-3_1m_000.mat'\n",
    "mat = scipy.io.loadmat(mat_path)\n",
    "print(mat['simpar'])\n",
    "print(mat['metapar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rir = mat['impulse_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape=torch.Size([8, 480000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agadi\\AppData\\Local\\Temp\\ipykernel_15444\\196252497.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.tensor(rir.T, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "a = torch.tensor(rir.T, dtype=torch.float32)\n",
    "print(f\"a.shape={a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 480000])\n"
     ]
    }
   ],
   "source": [
    "rir = a.flip(dims=(0, ))\n",
    "print(rir.shape)\n",
    "waveform = torchaudio.transforms.Resample(48000, 16000)(rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_mat_as_torch(mat_path: str) -> torch.tensor:\n",
    "    mat = scipy.io.loadmat(mat_path)\n",
    "    data_impulse_response = mat['impulse_response']\n",
    "    impulse_response_fs = 48000\n",
    "\n",
    "    rir = torch.tensor(data_impulse_response.T, dtype=torch.float32)\n",
    "    # Reorder microphones\n",
    "    if True:\n",
    "        rir = rir.flip((0, ))\n",
    "        \n",
    "    # 8 x XXX\n",
    "    waveform = torchaudio.transforms.Resample(impulse_response_fs, 16000)(rir)\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 160000])\n"
     ]
    }
   ],
   "source": [
    "wav1 = _load_mat_as_torch('timit_rir/Impulse_response_Acoustic_Lab_Bar-Ilan_University_(Reverberation_0.160s)_3-3-3-8-3-3-3_1m_000.mat')\n",
    "print(wav1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import config\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchaudio.transforms import Spectrogram\n",
    "NFFT=512\n",
    "HOP_LENGTH=128\n",
    "spectrogram = Spectrogram(\n",
    "    n_fft=NFFT,\n",
    "    win_length=NFFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    power=None  # Complex spectrum\n",
    ")\n",
    "\n",
    "def plot_all(stfts):\n",
    "    fig, axes = plt.subplots(len(stfts), 1, figsize=(10, 8))\n",
    "\n",
    "    for ax, stft in zip(axes, stfts):\n",
    "        stft_db = librosa.amplitude_to_db(abs(stft), ref=np.max)\n",
    "\n",
    "        print(stft_db.shape)\n",
    "        print(stft.shape)\n",
    "        spec = librosa.display.specshow(\n",
    "            stft_db,\n",
    "            sr=config.FS,\n",
    "            n_fft=config.NFFT,\n",
    "            win_length=config.NFFT,\n",
    "            hop_length=config.HOP_LENGTH,\n",
    "            y_axis='linear',\n",
    "            x_axis='time',\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        fig.colorbar(\n",
    "            spec,\n",
    "            ax=ax.axes,\n",
    "            format=\"%+2.0f dB\",\n",
    "            # orientation='horizontal'\n",
    "        )\n",
    "\n",
    "        ax.set_ylabel(\"Frequency [Hz]\")\n",
    "\n",
    "        # ax.set_xlim([0, duration])\n",
    "        ax.set_xlabel(\"Time [sec]\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_signals.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plot_all([\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperceived_signals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      3\u001b[0m           spectrogram(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperceived_signals\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()),\n\u001b[0;32m      4\u001b[0m           spectrogram(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixed_signals\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()),\n\u001b[0;32m      5\u001b[0m           d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m           d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms1\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\agadi\\miniconda3\\envs\\audio_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\agadi\\miniconda3\\envs\\audio_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\agadi\\miniconda3\\envs\\audio_env\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agadi\\miniconda3\\envs\\audio_env\\lib\\site-packages\\torchaudio\\functional\\functional.py:122\u001b[0m, in \u001b[0;36mspectrogram\u001b[1;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[0;32m    119\u001b[0m frame_length_norm, window_norm \u001b[38;5;241m=\u001b[39m _get_spec_norms(normalized)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# pack batch\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "d = torch.load('example_signals.pt')\n",
    "a = spectrogram(d['perceived_signals'][0][0].cpu().numpy())\n",
    "plot_all([a,\n",
    "          spectrogram(d['perceived_signals'][0][1].cpu().numpy()),\n",
    "          spectrogram(d['mixed_signals'][0][3].cpu().numpy()),\n",
    "          d['s0'],\n",
    "          d['s1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
